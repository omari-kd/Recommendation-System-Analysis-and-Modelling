---
title: "Recommendation System Analysis and Modelling"
author: "Omari Ebenezer"
output: html_notebook
---

------------------------------------------------------------------------

> ::: {#overview style="text-align: justify"}
> # **Introduction**
>
> Recommendation systems are essential for delivering personalised user experiences across a variety of platforms, including e-commerce, streaming services, social media and news websites.
>
> This project aims to develop a recommendation system that leveraged historical user data to provide tailored recommendations across different domains, such as product recommendations, content suggestions and service optimisation.
>
> ## **CRISP DM Framework**
>
> The analysis followed the CRISP-DM methodology, which includes the following stages:
>
> ### **1. Business Understanding:**
>
> The objectives were defined below, followed by the formulation of analytic questions to guide the modelling process.
>
> Key objectives of the project include:
>
> 1.Develop Personalized Recommendations: Tailor suggestions based on user behaviour and past interactions.
>
> 2.Address Diverse Use Cases: Implement systems for product, content and service recommendations.
>
> 3.Utilize Historical Data: Leverage past user actions to make accurate predictions.
>
> 4.Enhance User Engagement: Improve user satisfaction and retention through relevant suggestions.
>
> 5.Ensure Scalability & Real-Time Performance: Handle large data volumes and provide recommendations promptly.
>
> 6.Boost Business Metrics: Increase sales and conversion rates through better user personalization.
>
> 7.Balance Accuracy & Diversity: Provide relevant but varied recommendations to avoid monotony.
>
> \
> Analytic Questions:**\
> **
>
> ### **2. Data Understanding:**
>
> The dataset consists of three files: events.csv, item_properties.csv and category_tree.csv, which collectively describe the interactions and properties of items on an e-commerce website. The data, collected over a 4.5-month period, is raw and contains hashed values due to confidentiality concerns. The goal of publishing this dataset is to support research in recommender systems using implicit feedback.
>
> 2.1 Behaviour Data (events.csv)
>
> The behaviour data includes a total of 2,756,101 events, with 2,664,312 views, 69,332 add-to-cart actions, and 22,457 transactions, recorded from 1,407,580 unique visitors. Each event corresponds to one of three types of interactions: "view", "addtocart", or "transaction". These implicit feedback signals are crucial for recommender systems:
>
> View: Represents a user showing interest in an item.
>
> Add to Cart: Indicates a higher level of intent to purchase.
>
> Transaction: Represents a completed purchase.
>
> 2.2 Item Properties (item_properties.csv)
>
> This file contains 20,275,902 rows, representing various properties of 417,053 unique items. Each property may change over time (e.g., price updates), with each row capturing a snapshot of an item’s property at a specific timestamp. For items with constant properties, only a single snapshot is recorded. The file is split into two due to its size, and it contains detailed item information, which is essential for building item profiles and understanding how item properties influence user behaviour.
>
> 2.3 Category Tree (category_tree.csv)
> :::

### **3. Data Preparation:**

```{r}
# ----------------------------
# Load libraries
# ----------------------------
library(data.table)
library(dplyr)
library(tidyr)
library(ggplot2)
library(caret)
library(janitor)
library(reshape2)
library(scales)
library(lubridate)
library(isotree)  
```

```{r}

# -------------------------------------
# Chunk Function
# -------------------------------------

read_in_chunks <- function(file_path, chunk_size = 10000, ...) {
  # 1. Open the file connection for reading
  con <- file(file_path, open = "r")
  
  # Read the header line (assumes CSV header is in the first line)
  header <- readLines(con, n = 1)
  
  # Prepare an empty list to store chunks
  chunks <- list()
  chunk_index <- 1
  
  repeat {
    # Read the next chunk_size lines
    lines <- readLines(con, n = chunk_size)
    
    # Exit the loop if no more lines are available
    if (length(lines) == 0) break
    
    # Combine header and the chunk's lines to form a valid CSV text
    csv_text <- paste(c(header, lines), collapse = "\n")
    
    # Read the combined text into a data frame
    chunk_df <- read.csv(text = csv_text, header = TRUE, stringsAsFactors = FALSE, ...)
    
    # Store the chunk in the list
    chunks[[chunk_index]] <- chunk_df
    chunk_index <- chunk_index + 1
  }
  
  # Close the connection
  close(con)
  
  return(chunks)
}

```

```{r}
# Read category tree.csv
category_tree <- fread("./00_raw_data/category_tree.csv")
```

```{r}
# Read events.csv in chunks
events <- read_in_chunks("./00_raw_data/events.csv")
```

```{r}
# Read item_properties_part1.1.csv in chunks
item_properties_1 <- read_in_chunks("./00_raw_data/item_properties_part1.1.csv")
```

```{r}
# Read item_properties_part2.csv in chunks
item_properties_2 <- read_in_chunks("./00_raw_data/item_properties_part2.csv")
```

```{r}
# Concatenate the two item_properties 
# item_properites <- c(item_properties_1, item_properties_2)
```

> **Data Preprocessing for category_tree**

```{r}
# Check for duplicates and clean column names
sum(duplicated(category_tree))
```

```{r}
# Check for NA's 
colSums(is.na(category_tree) | category_tree == "")
```

```{r}
# Check the data type of each variable
str(category_tree)
```

> **Data Preprocessing for events**

```{r}
# Bind events to data frame
events <- rbindlist(events)
```

```{r}
# Check for duplicates in events
sum(duplicated(events))

# Remove duplicates 
events <- unique(events)

# Verify
sum(duplicated(events))
```

```{r}
# Check for NA's in events
colSums(is.na(events) | events == "")
```

```{r}
# Replace the NA's in the transactions for View and Add to cart since both don't involve any transactions
events$transactionid <- replace_na(events$transactionid, 0)
```

```{r}
# Check data type
str(events)
```

```{r}
# Convert to appropiate data types
# events$visitorid <- as.character(events$visitorid)

# events$itemid <- as.character(events$itemid)

# Convert 'event' to factor
events[, event := as.factor(event)]
```

```{r}
# Check datatype or class of the variable timestamp
class(events$timestamp)

# Convert to POSIXct# Calculate thresholds while removing NA values
events$timestamp <- as.POSIXct(events$timestamp / 1000, origin = "1970-01-01", tz = "UTC")
```

```{r}
# Data Visualisation
options(scipen = 999)

events_counts <- events %>% 
  group_by(event) %>% 
  summarise(count = n())

events_counts %>%
  ggplot(aes(x = event, y = count, fill = event)) + 
  geom_bar(stat = "identity", position = "dodge", color = NA, width = 0.7) +
  geom_text(aes(label = count), vjust = -0.5, color = "black") +
  ylim (0, 3000000) +
  labs(
    title = "Distribution of User Events",
    x = "Event Type",
    y = "Count"
  )

```

> [The chart above visualises the distribution of user events, addtocarts, transaction, view.]{style="text-align: justify"}
>
> [A. View Events: The view event category has the highest count by significant margin, with a total of 2,644, 218 events. This suggests visitors are viewing items on the platform.]{style="text-align: justify"}
>
> [B. Add to Cart Events: There are 68,966 events where visitors or users have added items to their cart. This is a positive sign as it indicates user interest in purchasing, although the number is much lower compared to the events.]{style="text-align: justify"}
>
> [C. Transaction Events: This category has the lowest count with 22,457 events. This represents the number of completed transactions, which is a critical metric for revenue generation.]{style="text-align: justify"}
>
> [Analysis:]{style="text-align: justify"}
>
> [The large number of view events compared to add to carts and transaction events suggest that there might be a drop-off in the conversion funnel (A conversion funnel, also known as a sales funnel or marketing funnel, is a visual representation of the customer journey from the first point of contact with a business to the final purchase or desired action. It's a model used to understand how potential customers move through different stages towards conversion, which is typically a sale but can also be any other desired action like signing up for a newsletter, downloading a whitepaper, purchasing product, et.) Users are viewing content but not as many are proceeding to add items to cart or complete transactions.]{style="text-align: justify"}
>
> [The ratio of view to transaction events is quite high. This could indicate that there are barriers to conversion, such as high prices, a poor user experience or lack of trust in the platform.]{style="text-align: justify"}
>
> [The add to cart events are significantly higher than transactions, which is expected, but the gap is quite large. This could mean that users are adding items to their carts but not completing the purchase. Reasons could include abandoned carts, issues during the checkout process, or the user deciding against the purchase at the last moment.]{style="text-align: justify"}

**Conversion Rates**

Integrating conversion rates in the analysis is key, as it helps to to segment users, compare against normal behaviour or as additional features in your recommendation or anomaly detection models.

Also tracking conversion rates can help you detect issues or improvements in your user funnel and overall system performance.

Lastly, If there is a suspiscion of some users are bots, you might compare their conversion rates with genuine users to see if there are significant differences, which further validates the bot detection.

```{r}
# Count the number of each event per visitor
user_event_counts <- dcast(events, visitorid ~ event, fun.aggregate = length, value.var = "event", fill = 0)
setnames(user_event_counts, old = c("view", "addtocart", "transaction"),
         new = c("num_view", "num_addtocart", "num_transaction"))


setDT(user_event_counts)
# Calculate conversion rates per visitor
user_event_counts[, conversion_view_to_add := ifelse(num_view > 0, num_addtocart / num_view, 0)]
user_event_counts[, conversion_add_to_transaction := ifelse(num_addtocart > 0, num_transaction / num_addtocart, 0)]
user_event_counts[, conversion_view_to_transaction := ifelse(num_view > 0, num_transaction / num_view, 0)]

# Inspect the calculated conversion rates
head(user_event_counts)
```

```{r}
# Histogram for the view-to-add conversion rate
ggplot(user_event_counts, aes(x = conversion_view_to_add)) +
  geom_histogram(binwidth = 0.05, fill = "steelblue", color = "black", na.rm = TRUE) +
  labs(
    title = "Distribution of View-to-Add Conversion Rate",
    x = "Conversion Rate (Add-to-View)",
    y = "Number of Users"
  ) +
  theme_minimal()
```

> ### **Abnormal Bot Users Detection**
>
> **Strategies for Bot Detection**
>
> **Event Frequency and Volume:**
>
> -   **High Activity:** Bots may generate a significantly higher number of events (e.g., views, add-to-carts, transactions) than typical users in a short time frame.
>
> -   **Event Density:** Calculate the number of events per minute or per session. Extremely high densities can signal bot behaviour.

```{r}

 # --------------------------------------
# Abnormally Detection
# --------------------------------------

# setnames
# setnames(events, c("timestamp", "visitorid", "event", "itemid", "transaction"))

# Calculate event frequency per visitor (total events, events per minute, etc.)
# Assuming events are sorted by timestamp for each visitor
setorder(events, visitorid, timestamp)

# Calculate the time difference between consecutive events for each visitor
events[, time_diff := as.numeric(difftime(timestamp, shift(timestamp), units = "secs")), by = visitorid]

# Aggregate statistics per visitor: total events, median time difference, etc.
visitor_stats <- events[, .(
  total_events = .N,
  avg_time_diff = mean(time_diff, na.rm = TRUE),
  median_time_diff = median(time_diff, na.rm = TRUE)
), by = visitorid]

# Identify potential bots:
# For instance, flag users with unusually high total events or very low average time difference.
# These thresholds may need tuning based on your data.
# Calculate thresholds while removing NA values
bot_threshold_events <- quantile(visitor_stats$total_events, 0.95, na.rm = TRUE)   # Top 5% activity
bot_threshold_time <- quantile(visitor_stats$avg_time_diff, 0.05, na.rm = TRUE)      # Bottom 5% time gaps

visitor_stats[, bot_flag := (total_events > bot_threshold_events) | (avg_time_diff < bot_threshold_time)]

# Inspect flagged potential bots
potential_bots <- visitor_stats[bot_flag == TRUE]
print(potential_bots)

# Print number of potential bots
print(paste("Number of potential bots: ", nrow(potential_bots)))

# Show top ten bots
top_ten_bots <- potential_bots[order(-total_events)][1:10]
print(top_ten_bots)
```

> **Time Difference Calculation:**\
> By computing the time difference between consecutive events, one can observe if some visitors have extremely short intervals between actions which is manchester uniteda common bot behaviours.
>
> **\
> Aggregation and Thresholds:**\
> Aggregating by visitor allows the computation of overall activity metrics (e.g., total events, average time between events). Setting thresholds based on quantiles (e.g., top 5% for total events, bottom 5% for average time difference) provides a starting point for flagging anomalies

```{r}

# ------------------------------
# Remove Bots
# -------------------------------

# Remove events associated with bot users from the events data
# clean_events_df <- events[!visitorid %in% potential_bots$visitorid]

# Verify how many events remain after removing bot-related events
# cat("Number of events after removing bots:", nrow(clean_events_df), "\n")

```

> **Isolation Forest for Anomaly Detection**
>
> This step adds a sophisticated, model-based layer of bot detection on top of the simple activity threshold.

```{r}
# ---------------------------------------
# Isolation Forest for Anomaly Detection
# ---------------------------------------

# Prepare features for the Isolation Forest model.
# Use the aggregated metrics: total_events, avg_time_diff, and median_time_diff.
features <- as.data.frame(visitor_stats[, .(total_events, avg_time_diff, median_time_diff)])

# Check structure of features to ensure all columns are numeric
str(features)

# Initialize and fit the Isolation Forest model.
iso_model <- isolation.forest(features, ntrees = 100, sample_size = nrow(features), seed = 42)

# Get predictions which include an anomaly score for each user.
predictions <- predict(iso_model, newdata = features)

# Debugging: Check the structure and content of predictions.
str(predictions)
head(predictions)

# If predictions is an atomic vector of numeric values, add it directly.
visitor_stats[, anomaly_score := predictions]

# Define an anomaly threshold: Flag users whose anomaly score is in the top 1% as anomalies.
anomaly_threshold <- quantile(predictions, 0.99, na.rm = TRUE)
visitor_stats[, anomaly_flag := ifelse(anomaly_score > anomaly_threshold, -1, 1)]

# Check the updated visitor_stats
print(head(visitor_stats))

```

```{r}
# -----------------------------
# STEP 6: Combine Detection Criteria
# -----------------------------
# Define thresholds for rule-based bot detection
bot_threshold_events <- quantile(visitor_stats$total_events, 0.95, na.rm = TRUE)
bot_threshold_time <- quantile(visitor_stats$avg_time_diff, 0.05, na.rm = TRUE)

# Create the bot_flag_rule column based on the thresholds
visitor_stats[, bot_flag_rule := (total_events > bot_threshold_events) | (avg_time_diff < bot_threshold_time)]


# Define a final flag for bots:
# A visitor is flagged as a bot if they meet either the rule-based criteria or the Isolation Forest criteria.
visitor_stats[, final_bot_flag := (bot_flag_rule == TRUE) | (anomaly_flag == -1)]

# Identify the final list of bot users.
bot_users <- visitor_stats[final_bot_flag == TRUE]
print(paste("Number of bots detected (combined):", nrow(bot_users)))

# -----------------------------
# STEP 7: Filter Out Bot Users from Events Data
# -----------------------------

# Remove all events associated with flagged bot users.
cleaned_events <- events[!visitorid %in% bot_users$visitorid]

# Optionally, check how many unique users remain.
print(paste("Remaining unique users after bot removal:", uniqueN(cleaned_events$visitorid)))

```

**Visualizing Bot Detection Results**

```{r}
# Density plot of anomaly scores
ggplot(visitor_stats, aes(x = anomaly_score, fill = factor(final_bot_flag))) +
  geom_density(alpha = 0.5) +
  labs(title = "Density of Anomaly Scores", x = "Anomaly Score", fill = "Bot Flag") +
  theme_minimal()
```

**Scatter Plot of User Activity versus Time Difference**

Examine the relationship between the number of events and the average time difference per user. Bots might show distinct patterns compared to regular users.

```{r}
# Assuming visitor_stats already contains total_events and avg_time_diff
ggplot(visitor_stats, aes(x = total_events, y = avg_time_diff)) +
  geom_point(alpha = 0.5, color = "purple") +
  scale_x_log10() +  # Log scale if there's a wide range in total_events
  labs(title = "Total Events vs. Average Time Difference per User",
       x = "Total Events (log scale)", y = "Average Time Difference (seconds)") +
  theme_minimal()

```

**Data Visualisation after Anomaly Detection**

```{r}
# Data Visualisation
options(scipen = 999)

events_counts <- cleaned_events %>% 
  group_by(event) %>% 
  summarise(count = n())

events_counts %>%
  ggplot(aes(x = event, y = count, fill = event)) + 
  geom_bar(stat = "identity", position = "dodge", color = NA, width = 0.7) +
  geom_text(aes(label = count), vjust = -0.5, color = "black") +
  ylim (0, 3000000) +
  labs(
    title = "Distribution of User Events After Bots Removal",
    x = "Event Type",
    y = "Count"
  )

```

The charts show the distribution of user events after removing bots. The events tracked are 'view', 'addtocart' and 'transaction'.

**Key Findings:**

-   **Views**:

    -   Before bot removal: 2,664,218

    -   After bot removal: 1,817,072

    -   **Drop**: 847,146 (31.8% reduction)

    -   **Implication**: A significant portion of views were bot-generated, indicating inflated traffic metrics.

-   **Addtocart**:

    -   Before bot removal: 68,966

    -   After bot removal: 22,947

    -   **Drop**: 46,019 (66.7% reduction)

    -   **Implication**: Bots contributed to addtocart actions, but less so compared to views.

-   **Transactions**:

    -   Before bot removal: 22,457

    -   After bot removal: 5,392

    -   **Drop**: 17,065 (75.9% reduction)

    -   **Implication**: Transactions were less affected by bots, suggesting more genuine human actions.

Analysis:

The large drop in views suggests that the website's traffic was significantly inflated by bots, which could affect advertising revenue and perceived popularity.

The drop in addtocart events indicates that user engagement might be lower than initially thought, suggesting a need to improve the user experience or product offerings.

The 75.9% reduction means that bots contributed heavily to transaction counts, but the remaining transactions after bot removal are likely more reliable as genuine human activity. This makes transactions a more trustworthy metric for evaluating real user behaviour, even though the absolute drop was large.

**Data Visualisation of Top Most Viewed Products**

```{r}
# Filter the events data for view events only
view_events <- cleaned_events[event == "view"]

# Aggregate the number of views per product (itemid)
product_view_counts <- view_events[, .(view_count = .N), by = itemid]

# Order by view_count in descending order
setorder(product_view_counts, -view_count)

# Select the top 10 most viewed products
top_viewed_products <- head(product_view_counts, 10)

# Create a bar plot with ggplot2
ggplot(top_viewed_products, aes(x = reorder(itemid, view_count), y = view_count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 10 Most Viewed Products",
    x = "Product (itemid)",
    y = "View Count"
  ) +
  theme_minimal()

```

**Data Visualisation of Top Most Purchased Products**

```{r}

# Filter the events data for transaction events only
purchase_events <- cleaned_events[event == "transaction"]

# Aggregate the number of purchases per product (itemid)
product_purchase_counts <- purchase_events[, .(purchase_count = .N), by = itemid]

# Order by purchase_count in descending order
setorder(product_purchase_counts, -purchase_count)

# Select the top 10 most purchased products
top_purchased_products <- head(product_purchase_counts, 10)

# Create a bar plot with ggplot2
ggplot(top_purchased_products, aes(x = reorder(itemid, purchase_count), y = purchase_count)) +
  geom_bar(stat = "identity", fill = "forestgreen") +
  coord_flip() +  # Flip axes for better readability
  labs(
    title = "Top 10 Most Purchased Products",
    x = "Product (itemid)",
    y = "Purchase Count"
  ) +
  theme_minimal()

```

**Conversion funnel visualisation**

Show how many users progress through each stage of your funnel—from view to add-to-cart to transaction

```{r}
# Aggregate counts for each event type using the cleaned events (if desired)
funnel_data <- cleaned_events[, .(
  num_view = sum(event == "view"),
  num_addtocart = sum(event == "addtocart"),
  num_transaction = sum(event == "transaction")
)]

# Convert the data to long format for plotting
funnel_long <- melt(funnel_data, measure.vars = c("num_view", "num_addtocart", "num_transaction"),
                    variable.name = "stage", value.name = "count")

ggplot(funnel_long, aes(x = stage, y = count)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Conversion Funnel", x = "Stage", y = "Event Count") +
  theme_minimal()
```

**Data Visualisation of Time Series of Events Over Time**

```{r}
# Aggregate events by day (or week)
daily_events <- cleaned_events[, .(total_events = .N), by = .(date = as.Date(timestamp))]
  
ggplot(daily_events, aes(x = date, y = total_events)) +
  geom_line(color = "blue") +
  labs(title = "Daily Total Events", x = "Date", y = "Number of Events") +
  theme_minimal()
```

**Data Visualisation of Distribution of Time Differences**

```{r}

ggplot(cleaned_events[!is.na(time_diff)], aes(x = time_diff)) +
  geom_histogram(binwidth = 10, fill = "coral", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Time Differences Between Events", 
       x = "Time Difference (seconds)", y = "Frequency") +
  theme_minimal()

```

**Heatmap of Event Activity by Hour and Day of Week**

```{r}
# Extract hour and day of week from datetime
cleaned_events[, hour := hour(datetime)]
cleaned_events[, weekday := weekdays(datetime)]

# Aggregate event counts by hour and weekday
heatmap_data <- cleaned_events[, .(count = .N), by = .(weekday, hour)]

# Reorder weekdays (optional, for proper ordering)
heatmap_data[, weekday := factor(weekday, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))]

ggplot(heatmap_data, aes(x = hour, y = weekday, fill = count)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "red") +
  labs(title = "Heatmap of Events by Hour and Weekday", x = "Hour of Day", y = "Weekday") +
  theme_minimal()

```

**\
Data Preprocessing for item_properties**

```{r}
# Bind item properties 1 & 2 in a data frame
item_properties <- do.call(rbind, c(item_properties_1, item_properties_2))
```

```{r}
# Data Cleaning for item properties
# Check for duplicates in item_properties
# sum(duplicated(item_properties))
```

```{r}
# Check for NA's in item properties
# colSums(is.na(item_properties) | item_properties == "")
```

```{r}
# Convert to POSIXct
item_properties$timestamp <- as.POSIXct(item_properties$timestamp / 1000, origin = "1970-01-01", tz = "UTC")
```

```{r}
# 1. Keep only the latest property value for each item
# Sort by 'itemid' and 'timestamp', then group by itemid and keep the lastest row
item_properties <- item_properties %>%
  arrange(itemid, timestamp) %>%
  group_by(itemid) %>%
  slice_tail(n = 1) %>%
  ungroup()

# 2. Filter for category and availability properties
item_properties_filtered <- item_properties %>%
  filter(property %in% c("categoryid", "available"))

# 3. Restructure or pivot the data so that each item has its categoryid & available as separate columns
item_properties_restructure <- item_properties_filtered %>%
  pivot_wider(names_from = property, values_from = value)

# 4. Convert availability to numeric and then to integer (0 or 1)
# First, convert to numeric and replace NAs with 0
item_properties_restructure <- item_properties_restructure %>%
  mutate(available = as.numeric(available)) 

# Replace NA's in 'available' with 0, then convert to integer
item_properties_restructure$available[is.na(item_properties_restructure$available)] <- 0
item_properties_restructure <- item_properties_restructure %>%
  mutate(available = as.integer(available))
```

```{r}
# Clean numeric values
# Use string functions to remove the prefix and then convert them to numeric.
# Some of the 'value' column sometimes starts with "n"

# Check Class
# class(item_properties)

# Set to data.table
# setDT(item_properties)

# Apply the operation
# item_properties[, value := trimws(value)]
# item_properties[, value_clean := as.numeric(gsub("^n", "", value))]

# item_properties[, value_clean := as.numeric(gsub("[^0-9.-]", "", gsub("^n", "", value)))]

# item_properties[, value_clean := as.numeric(gsub("^n+", "", value))]


# Round to 3 decimanls for precision
# item_properties[, value_clean := round(value_clean, 3)]

# extract_numeric_value <- function(x) {
  # Split the string by whitespace
  # tokens <- strsplit(x, " ")[[1]]
  
  # Find tokens that start with "n" (one or more)
  # n_tokens <- grep("^n+", tokens, value = TRUE)
  
  # if (length(n_tokens) > 0) {
    # If multiple tokens start with n, choose the first one (or apply your logic here)
    # Remove the "n" prefix(s) and convert to numeric
    # return(as.numeric(gsub("^n+", "", n_tokens[1])))
  # } else {
    # Optionally: if no token with "n" is found, you could decide to return NA or try to convert the first token
    # return(as.numeric(tokens[1]))
  # }
# }

# Apply the function to each row in the 'value' column and store the result in a new column 'value_clean'
item_properties[, value_clean := sapply(value, extract_numeric_value)]


# Check conversion
summary(item_properties$value_clean)


# Even though many values in the file are hashed to protect confidentiality, not all values are treated the same way. 

# Text or categorical values are hashed to anonymize the data. These values aren’t meant to be interpreted numerically.

# Certain properties like price or other quantitative attributes are meant to be used in calculations. These numeric values are stored with an "n" prefix (e.g., "n5.000") to denote that they are numbers with a specific precision. They are not hashed; they're just formatted as strings with that prefix.
```

> ```{r}
> >
> # For columns like property that are hashed, ensure that the same hash represents the same property across different rows.
> # Group by itemid and property to see if there are any inconsistencies.
> >
> # Count distinct value entries for each combination of item and property
> # consistency_check <- item_properties[, .(unique_values = uniqueN(value_clean)), by = .(itemid, property)]
> >
> # If a property is expected to be constant, unique_values should be 1.
> # summary(consistency_check$unique_values)
> >
> ```
>
> ```{r}
> >
> # Merging datasets
> # Perform a Rolling Join
> # A rolling join allows you to match each event with the most recent (previous) item property snapshot.
> >
> # Set event to data.table
> setDT(events)
> >
> # Order the data by itemid and timestamp
> setorder(events, itemid, timestamp)
> setorder(item_properties, itemid, timestamp)
> >
> # Set keys for a rolling join
> setkey(events, itemid, timestamp)
> setkey(item_properties, itemid, timestamp)
> >
> # Rolling join: for each event, get the most recent snapshot from item properties.
> # This matches on 'itemid' and finds the snapshot with a timestamp less than or equal to the event timestamp.
> # merged_data <- item_properties[events, on = .(itemid, timestamp), roll = TRUE]
> >
> >
> merged_data <- events[item_properties, on = .(itemid, timestamp), roll = TRUE]
> >
> # Inspect the merged result
> head(merged_data)
> >
> # Both events and item_props are keyed by itemid and timestamp. This ensures the join is performed efficiently.
> >
> # Rolling Join (roll = TRUE): When you join item_props  with events, the roll = TRUE option tells data.table to find, for each event, the row in item_props with the closest timestamp that does not exceed the event's timestamp. This aligns each event with the proper snapshot of the item properties.
> ```
>
> ```{r}
> # Filter the item properties data to isolate the rows where the property is "categoryid". This gives you the actual category identifier for each item.
> >
> category_property <- item_properties[property == "categoryid"]
> >
> # Perform a rolling join Since item properties are time-dependent, align each event with the most recent "categoryid" snapshot preceding the event time
> >
> # Order and set keys for rolling join
> setorder(category_property, itemid, timestamp)
> setorder(events, itemid, timestamp)
> setkey(category_property, itemid, timestamp)
> setkey(events, itemid, timestamp)
> >
> >
> # Rolling join: For each event, get the most recent "categoryid" snapshot
> events_with_category <- category_property[events, on = .(itemid, timestamp), roll = TRUE]
> >
> # Rolling Join (roll = TRUE): When you join item_props with events, the roll = TRUE option tells data.table to find, for each event, the row in item_props with the closest timestamp that does not exceed the event's timestamp. This aligns each event with the proper snapshot of the item properties
> ```
>
> ```{r}
> # Rename and Prepare the Category Identifier
> # For clarity, rename the column containing the category identifier. 
> events_with_category[, categoryid := value_clean]
> >
> >
> # Merge with category tree
> # setKey
> setkey(category_tree, categoryid)
> setkey(events_with_category, categoryid)
> >
> >
> # Merge the category tree with the events data
> final_data <- merge(events_with_category, category_tree, by = "categoryid", all.x = TRUE)
> >
> >
> # In the merge() function in R, the all.x = TRUE argument specifies that the merge should be left join (keeping all rows from the left dataset and only matching rows from the right dataset).
> ```
